\section{Introduction}

The foundation for interpreting neural network activations as indicators of feature strength can be traced back to the pioneering work of McCulloch and Pitts in 1943 \citep{mcculloch1943logical}, who introduced the concept of artificial neurons with a threshold for activation. \footnote{The implementation for this work can be found at \url{https://github.com/alanoursland/neural_networks_use_distance_metrics}.} This concept, where larger outputs signify stronger representations, was further developed by Rosenblatt's 1958 perceptron model \citep{rosenblatt1958perceptron} and has persisted through the evolution of neural networks and deep learning \citep{schmidhuber2015deep}. However, despite the remarkable success achieved through this lens, the statistical principles underlying neural network feature learning remain incompletely understood \citep{lipton2018mythos}. We refer to this "larger is stronger" interpretation as an "intensity metric." \citep{simonyan2013deep}

This work builds on our recent theoretical framework \citep{oursland2024interpreting} that proposed neural networks might naturally learn to compute statistical distance metrics, specifically the Mahalanobis distance \citep{mahalanobis1936generalized}. This viewpoint suggests that smaller node activations correspond to stronger feature representation. While this previous work established a mathematical relationship between neural network linear layers and the Mahalanobis distance, the existence of the relationship does not necessarily imply that networks employ these representations in practice. 

We use systematic perturbation analysis \citep{szegedy2013intriguing, goodfellow2014explaining} to provide empirical evidence supporting the distance metric theory proposed in our previous work. Using the MNIST dataset \citep{lecun1998gradient}, we modify trained models by independently manipulating distance and intensity properties of network activations. By analyzing how these perturbations affect model performance, we identify the key properties (distance vs. intensity) that drive network behavior. Our investigation focuses on two key questions:

\begin{itemize}
    \item Do neural networks naturally learn to measure distances rather than intensities when processing data distributions?
    \item How do different activation functions (ReLU and Absolute Value) affect the type of statistical measures learned by the network?
\end{itemize}

Our results show that networks with both ReLU and Absolute Value activations are highly sensitive to distance-based perturbations while maintaining robust performance under intensity perturbation supporting the hypothesis that they primarily learn distance-based metrics. These findings not only validate our theoretical framework but also suggest new approaches for understanding and improving neural network architectures.