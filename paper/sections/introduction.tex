\section{Introduction}

Rosenblatt's 1958 perceptron pioneered the use of activation values as confidence measures, establishing a paradigm where larger neural network outputs signify stronger feature representations. While this interpretation has supported remarkable achievements across diverse applications, the statistical principles underlying neural network feature learning remain incompletely understood.

This work builds on our recent theoretical framework [Oursland, 2024] that proposed neural networks might naturally learn to compute statistical distance metrics, particularly the Mahalanobis distance. While this previous work established a mathematical relationship between neural network linear layers and this statistical distance measure, offering a more rigorous interpretation of their operations, the existence of such a relationship does not necessarily imply that networks employ these computations in practice. This paper presents empirical evidence to validate these theoretical predictions through systematic perturbation analysis.

This paper presents empirical evidence supporting the distance metric interpretation through systematic perturbation analysis. Using the MNIST dataset, we implement controlled perturbation experiments that independently manipulate distance and intensity properties of network activations. By analyzing how these perturbations affect model performance, we determine which properties drive network behavior. Our investigation focuses on two key questions:

1. Do neural networks naturally learn to measure distances rather than intensities when processing data distributions?
2. How do different activation functions (ReLU and Absolute Value) affect the type of statistical measures learned by the network?

Our results demonstrate that networks with both ReLU and Absolute Value activations show strong sensitivity to distance-based perturbations while maintaining robust performance under intensity scaling, supporting the hypothesis that they primarily learn distance-based features. These findings not only validate our theoretical framework but also suggest new approaches for understanding and improving neural network architectures.