\section{Introduction}

The foundation for interpreting neural network activations as indicators of feature strength can be traced back to the pioneering work of McCulloch and Pitts in 1943 \citep{mcculloch1943logical}, who introduced the concept of artificial neurons with a threshold for activation. \footnote{The implementation for this work can be found at \url{https://github.com/alanoursland/neural_networks_use_distance_metrics}.} This concept, where larger outputs signify stronger representations, was further developed by Rosenblatt's 1958 perceptron model \citep{rosenblatt1958perceptron} and has persisted through the evolution of neural networks and deep learning \citep{schmidhuber2015deep}. Throughout this evolution, the field has largely maintained this interpretation that larger activation values indicate stronger feature presence -- what we term an \emph{intensity metric}. However, despite the remarkable success achieved through this lens, the statistical principles underlying neural network feature learning remain incompletely understood \citep{lipton2018mythos}.

This work builds on our recent theoretical framework \citep{oursland2024interpreting} that proposed neural networks might naturally learn to compute statistical \emph{distance metrics}, specifically the Mahalanobis distance \citep{mahalanobis1936generalized}. Our analysis suggested that smaller node activations, rather than larger ones, might correspond to stronger feature representations. While this previous work established a mathematical relationship between neural network linear layers and the Mahalanobis distance, we need empirical evidence to determine whether networks actually employ these distance-based representations in practice.

We use systematic perturbation analysis \citep{szegedy2013intriguing, goodfellow2014explaining} to provide empirical evidence supporting the distance metric theory proposed in our previous work. Using the MNIST dataset \citep{lecun1998gradient}, we modify trained models by independently manipulating distance and intensity properties of network activations. By analyzing how these perturbations affect model performance, we identify which properties -- distance or intensity -- drive network behavior. Our investigation focuses on two key questions:

\begin{itemize}
    \item Do neural networks naturally learn to measure distances rather than intensities when processing data distributions?
    \item How do different activation functions (ReLU and Absolute Value) affect the type of statistical measures learned by the network?
\end{itemize}

Our results show that networks with both ReLU and Absolute Value activations are highly sensitive to distance-based perturbations while maintaining robust performance under intensity perturbations, supporting the hypothesis that they utilize distance-based metrics. These findings not only validate our theoretical framework but also suggest new approaches for understanding and improving neural network architectures.