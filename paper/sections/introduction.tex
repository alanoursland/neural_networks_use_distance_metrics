\section{Introduction}

Rosenblatt's 1958 perceptron model pioneered the use of activation values as confidence or intensity measures, establishing a paradigm where larger neural network outputs signify stronger feature representations. While this interpretation has supported remarkable achievements across diverse applications, the statistical principles underlying neural network feature learning remain incompletely understood.

This work builds on our recent theoretical framework \cite{oursland2024interpreting} that proposed neural networks might naturally learn to compute statistical distance metrics, particularly the Mahalanobis distance. This viewpoint suggests that smaller node activations correspond to stronger feature representation. While this previous work established a mathematical relationship between neural network linear layers and the Mahalanobis distance, the existence of this relationship does not necessarily imply that networks employ these representations in practice. 

We use systematic perturbation analysis to provide empirical evidence supporting the distance metric theory proposed in our previous work. Using the MNIST dataset, we modify trained models by independently manipulating distance and intensity properties of network activations. By analyzing how these perturbations affect model performance, we identify the key properties (distance vs. intensity) that drive network behavior. Our investigation focuses on two key questions:

\begin{itemize}
    \item Do neural networks naturally learn to measure distances rather than intensities when processing data distributions?
    \item How do different activation functions (ReLU and Absolute Value) affect the type of statistical measures learned by the network?
\end{itemize}

Our results show that networks with both ReLU and Absolute Value activations are highly sensitivity to distance-based perturbations while maintaining robust performance under intensity perturbation supporting the hypothesis that they primarily learn distance-based metrics. These findings not only validate our theoretical framework but also suggest new approaches for understanding and improving neural network architectures.