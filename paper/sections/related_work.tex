\section{Prior Work}

In 1943 McCulloch and Pitt introduced a computation model of a neuron to explore logical equations in biological brains \citep{mcculloch1943logical}. Their definition $\text{TRUE} = (Wx > b)$ marks the beginning of our path using intensity metrics. Rosenblatt adapted this into an activation value $y = f(Wx + b)$ in 1957 with the perceptron, further solidifying the intensity metric interpretation \citep{rosenblatt1957perceptron}.

The development of multilayer perceptrons (MLPs) and the backpropagation algorithm enabled the training of deeper networks with continuous activation functions. \citep{rumelhart1986learning,lecun1989backpropagation,hornik1989multilayer} The interpretation of activations continued to focus on larger values as being more salient, reflected in visualizations of activations and analyses of feature maps, where stronger activations were highlighted. \citep{zeiler2014visualizing,yosinski2015understanding,olah2017feature,erhan2009visualizing}

The rise of deep learning, with the widespread adoption of ReLU and its variants, further reinforced the intensity metric interpretation by emphasizing the importance of large, positive activations. \citep{nair2010rectified,glorot2011deep} Visualization techniques, such as saliency maps and Class Activation Mapping (CAM), often focused on highlighting regions with high activations. \citep{simonyan2013deep,zhou2016learning} Similarly, attention mechanisms, which assign weights to different parts of the input, often rely on the magnitude of these weights as indicators of importance. \citep{bahdanau2014neural,vaswani2017attention}

While the intensity metric interpretation has been dominant, recent work has highlighted its limitations. \citep{rudin2019stop} Considering the relationships between activations, particularly through distance metrics, offers a promising avenue for understanding neural network representations. \citep{goodfellow2014explaining,madry2017towards,szegedy2013intriguing} Distance-based methods, such as Radial Basis Function (RBF) networks and Siamese networks, demonstrate the potential of incorporating distance computations into neural network architectures and interpretation. \citep{broomhead1988radial,bromley1994signature,schroff2015facenet} This approach could lead to more nuanced and effective representations.