\section{Prior Work}

The interpretation of activations in neural networks has a rich history, closely connected to the evolution of these models. \cite{lecun1998gradient} From the earliest days, the notion that larger activations signify stronger feature representations has been prevalent, shaping our understanding and analysis of these complex systems. \cite{erhan2009visualizing, lecun1998gradient,  mcculloch1943logical, rosenblatt1958perceptron,  rumelhart1986learning} This interpretation, where the magnitude of activation directly reflects the strength of a feature or the confidence in its presence, has been a guiding principle in neural network design and analysis. \cite{zeiler2014visualizing,yosinski2015understanding,olah2017feature}  We refer to this interpretation as an "intensity metric." \cite{simonyan2013deep}

The foundation for this "larger is stronger" interpretation can be traced back to the pioneering work of McCulloch and Pitts \cite{mcculloch1943logical}, who introduced the concept of artificial neurons with a threshold for activation. \cite{erhan2009visualizing, lecun1998gradient, mcculloch1943logical, rosenblatt1958perceptron, rumelhart1986learning} Their work laid the groundwork for associating higher activation with a stronger response to a stimulus. \cite{rosenblatt1958perceptron, rumelhart1986learning, lecun1998gradient} This notion was further solidified by Rosenblatt's perceptron \cite{rosenblatt1958perceptron}, which explicitly linked larger activation values with the presence of a feature. \cite{rosenblatt1958perceptron,olah2017feature}

The development of multilayer perceptrons (MLPs) and the backpropagation algorithm \cite{rumelhart1986learning} enabled the training of deeper networks with continuous activation functions. \cite{lecun1989backpropagation,hornik1989multilayer,glorot2011deep} While this opened up new possibilities for representation learning, the interpretation of activations often still focused on larger values as being more salient. \cite{lecun1989backpropagation,hornik1989multilayer,glorot2011deep} This was reflected in visualizations of activations and analyses of feature maps, where stronger activations were highlighted. \cite{zeiler2014visualizing,yosinski2015understanding}

The advent of deep learning further reinforced the "larger is stronger" interpretation. \cite{krizhevsky2012imagenet,nair2010rectified,simonyan2013deep,zhou2016learning,bahdanau2014neural,vaswani2017attention} The widespread adoption of ReLU and its variants \cite{nair2010rectified,glorot2011deep,krizhevsky2012imagenet} implicitly emphasized the importance of large, positive activations. \cite{krizhevsky2012imagenet,nair2010rectified,simonyan2013deep,zhou2016learning,bahdanau2014neural,vaswani2017attention} Visualization techniques, such as saliency maps \cite{simonyan2013deep} and Class Activation Mapping (CAM) \cite{zhou2016learning}, often focused on highlighting regions with high activations. \cite{krizhevsky2012imagenet,nair2010rectified,simonyan2013deep,zhou2016learning,bahdanau2014neural,vaswani2017attention} Similarly, attention mechanisms, which assign weights to different parts of the input, often rely on the magnitude of these weights as indicators of importance. \cite{krizhevsky2012imagenet,nair2010rectified,simonyan2013deep,zhou2016learning,bahdanau2014neural,vaswani2017attention}

However, this prevailing focus on activation magnitude may overlook the crucial relational information encoded in the activation space. \cite{goodfellow2014explaining,madry2017towards,szegedy2013intriguing} The distances between activations, rather than just their absolute values, could hold valuable insights into how neural networks represent and process information. \cite{goodfellow2014explaining,madry2017towards,szegedy2013intriguing}

Distance-based methods, such as Radial Basis Function (RBF) networks \cite{broomhead1988radial} and Siamese networks \cite{bromley1994signature,schroff2015facenet}, explicitly utilize distance computations for tasks like classification and similarity learning. \cite{weinberger2009distance,von2007tutorial} Their success suggests that incorporating distance metrics into neural network architectures could offer a more nuanced and potentially more effective approach to feature representation and learning. \cite{weinberger2009distance,von2007tutorial}

This work builds upon these alternative perspectives, aiming to provide a more nuanced understanding of how neural networks represent and process information by exploring the role of distance metrics. \cite{oursland2024} We investigate whether neural networks might fundamentally operate as distance-measuring devices, rather than solely as feature detectors. \cite{oursland2024}