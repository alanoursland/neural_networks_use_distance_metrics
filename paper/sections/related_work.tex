\section{Prior Work}

The interpretation of activations in neural networks has a rich history, closely connected to the evolution of these models.  [cite: 14] From the earliest days, the notion that larger activations signify stronger feature representations has been prevalent, shaping our understanding and analysis of these complex systems. [cite: 14, 15, 16, 17, 18, 19] This interpretation, where the magnitude of activation directly reflects the strength of a feature or the confidence in its presence, has been a guiding principle in neural network design and analysis. [cite: 16, 42, 43] We refer to this interpretation as an "intensity metric." [cite: 17]

The foundation for this "larger is stronger" interpretation can be traced back to the pioneering work of McCulloch and Pitts \cite{mcculloch1943logical}, who introduced the concept of artificial neurons with a threshold for activation. [cite: 14, 15, 16, 17, 18, 19] Their work laid the groundwork for associating higher activation with a stronger response to a stimulus. [cite: 17, 18, 19] This notion was further solidified by Rosenblatt's perceptron \cite{rosenblatt1958perceptron}, which explicitly linked larger activation values with the presence of a feature. [cite: 19, 42]

The development of multilayer perceptrons (MLPs) and the backpropagation algorithm \cite{rumelhart1986learning} enabled the training of deeper networks with continuous activation functions. [cite: 20, 21, 22, 23] While this opened up new possibilities for representation learning, the interpretation of activations often still focused on larger values as being more salient. [cite: 20, 21, 22, 23] This was reflected in visualizations of activations and analyses of feature maps, where stronger activations were highlighted. [cite: 22, 23]

The advent of deep learning further reinforced the "larger is stronger" interpretation. [cite: 24, 25, 26, 27, 28, 29, 30] The widespread adoption of ReLU and its variants \cite{nair2010rectified,glorot2011deep,krizhevsky2012imagenet} implicitly emphasized the importance of large, positive activations. [cite: 24, 25, 26, 27, 28, 29, 30] Visualization techniques, such as saliency maps \cite{simonyan2013deep} and Class Activation Mapping (CAM) \cite{zhou2016learning}, often focused on highlighting regions with high activations. [cite: 24, 25, 26, 27, 28, 29, 30] Similarly, attention mechanisms, which assign weights to different parts of the input, often rely on the magnitude of these weights as indicators of importance. [cite: 24, 25, 26, 27, 28, 29, 30]

However, this prevailing focus on activation magnitude may overlook the crucial relational information encoded in the activation space. [cite: 35, 36, 37] The distances between activations, rather than just their absolute values, could hold valuable insights into how neural networks represent and process information. [cite: 35, 36, 37]

Distance-based methods, such as Radial Basis Function (RBF) networks \cite{broomhead1988radial} and Siamese networks \cite{bromley1994signature,schroff2015facenet}, explicitly utilize distance computations for tasks like classification and similarity learning. [cite: 35, 36, 37] Their success suggests that incorporating distance metrics into neural network architectures could offer a more nuanced and potentially more effective approach to feature representation and learning. [cite: 35, 36, 37]

This work builds upon these alternative perspectives, aiming to provide a more nuanced understanding of how neural networks represent and process information by exploring the role of distance metrics. [cite: 38, 39, 40, 58, 59, 60, 61, 62] We investigate whether neural networks might fundamentally operate as distance-measuring devices, rather than solely as feature detectors. [cite: 38, 39, 40, 58, 59, 60, 61, 62]