\section{Prior Work}

The interpretation of activations in neural networks has a rich history, closely connected to the evolution of these models. \citep{lecun1998gradient} From the earliest days, the notion that larger activations signify stronger feature representations has been prevalent, shaping our understanding and analysis of these complex systems. \citep{erhan2009visualizing, lecun1998gradient, mcculloch1943logical, rosenblatt1958perceptron, rumelhart1986learning} This interpretation, where the magnitude of activation directly reflects the strength of a feature or the confidence in its presence, has been a guiding principle in neural network design and analysis. \citep{zeiler2014visualizing,yosinski2015understanding,olah2017feature}  We refer to this interpretation as an "intensity metric." \citep{simonyan2013deep}

The foundation for this "larger is stronger" interpretation can be traced back to the pioneering work of McCulloch and Pitts \citep{mcculloch1943logical}, who introduced the concept of artificial neurons with a threshold for activation. Their work laid the groundwork for associating higher activation with a stronger response to a stimulus. This notion was further solidified by Rosenblatt's perceptron \citep{rosenblatt1958perceptron}, which explicitly linked larger activation values with the presence of a feature. \citep{rosenblatt1958perceptron,olah2017feature}

The development of multilayer perceptrons (MLPs) and the backpropagation algorithm \citep{rumelhart1986learning} enabled the training of deeper networks with continuous activation functions. \citep{lecun1989backpropagation,hornik1989multilayer,glorot2011deep} While this opened up new possibilities for representation learning, the interpretation of activations often still focused on larger values as being more salient. This was reflected in visualizations of activations and analyses of feature maps, where stronger activations were highlighted. \citep{zeiler2014visualizing,yosinski2015understanding}

The advent of deep learning further reinforced the "larger is stronger" interpretation. \citep{krizhevsky2012imagenet} The widespread adoption of ReLU and its variants \citep{nair2010rectified,glorot2011deep} implicitly emphasized the importance of large, positive activations.  Visualization techniques, such as saliency maps \citep{simonyan2013deep} and Class Activation Mapping (CAM) \citep{zhou2016learning}, often focused on highlighting regions with high activations. Similarly, attention mechanisms, which assign weights to different parts of the input \citep{bahdanau2014neural,vaswani2017attention}, often rely on the magnitude of these weights as indicators of importance. 

Visualization techniques, such as saliency maps and Class Activation Mapping (CAM), often focused on highlighting regions with high activations. Similarly, attention mechanisms, which assign weights to different parts of the input, often rely on the magnitude of these weights as indicators of importance.

While the "larger is stronger" interpretation has been dominant, recent work has highlighted the limitations of focusing solely on the magnitude of individual activations. \citep{rudin2019stop} Considering the relationships between activations, particularly through distance metrics, offers a promising avenue for understanding neural network representations. \citep{goodfellow2014explaining,madry2017towards,szegedy2013intriguing} Distance-based methods, such as Radial Basis Function (RBF) networks \citep{broomhead1988radial} and Siamese networks \citep{bromley1994signature,schroff2015facenet}, explicitly utilize distance computations for tasks like classification and similarity learning. Their success suggests that incorporating distance computations into neural network architectures and interpretation could lead to more nuanced and effective representations, offering a more effective approach to feature representation and learning.

This work explores the potential of distance-based methods to unlock a deeper understanding of neural network representations, moving beyond the traditional focus on individual activations.