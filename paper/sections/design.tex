\section{Experimental Design}

To empirically investigate whether neural networks naturally learn distance-based features, we designed systematic perturbation experiments that differentiate between distance-based and intensity-based feature learning. Our experimental framework enables direct comparison between these two interpretations by examining how learned features respond to specific modifications of their activation patterns.

\subsection{Model Architecture}
We implemented a simple feedforward neural network architecture to test our hypotheses. The network processes MNIST digits through the following layers:
\begin{enumerate}
    \item Input layer (784 units, flattened $28 \times 28$ images)
    \item Linear layer (128 units)
    \item Perturbation layer
    \item Activation function
    \item Output layer (10 units)
\end{enumerate}

The perturbation layer is a custom module that allows precise control over the activation patterns through two fixed parameters: a multiplicative factor $s$ and a translational offset $t$. During training, these parameters are held constant ($s=1$, $t=0$) so the layer has no effect on the network's learning. After training, we can modify these parameters to probe the network's learned features. For any input $x$, the perturbation layer computes:

\begin{equation}
    \text{output} = sx + t
\end{equation}

where $s$ and $t$ can be set independently for each unit in the layer to create controlled modifications of the activation patterns.

\subsection{Training Protocol}
We trained on MNIST using standard classification procedures. For convenience and speed, we use the entire training set for each parameter update instead of minibatch training. This necessitates a larger number of epochs (5000) since we only update parameters once per epoch. Other training parameters include:
\begin{itemize}
    \item Optimizer: SGD with learning rate 0.001
    \item Loss function: Cross-entropy loss
    \item Data normalization: $\mu = 0.1307$, $\sigma = 0.3081$
\end{itemize}

To ensure robust results, we repeated each experiment 20 times with different random initializations.

\subsection{Perturbation Design}
The core of our experimental design lies in two carefully crafted perturbation types that differentially affect distance-based and intensity-based features.

\subsubsection{Distance Perturbation}
The distance perturbation is designed to affect small activation values while preserving large ones. For each node, we:
\begin{enumerate}
    \item Calculate its natural activation range: $r = \text{max}{\text{activation}} - \text{min}{\text{activation}}$
    \item Scale the activation by $(1-p)$ to maintain the maximum value
    \item Apply a relative shift $p \cdot r$ (as a percentage of the range)
\end{enumerate}

For a given percentage $p$ and range $r$, the perturbation parameters are:
\begin{align}
a &= 1 - p \label{eq:scale} \
b &= pr \label{eq:offset}
\end{align}

This perturbation specifically targets distance-like features by shifting their "zero point" (the location of best matches) while maintaining the overall dynamic range. If the model is using distance metric features, small shifts near zero should significantly impact accuracy, as this moves the decision boundary away from true matches.

\subsubsection{Intensity Perturbation}
The intensity perturbation is a pure scaling operation that affects large activation values while preserving near-zero values. For a scale factor $s$:
\begin{align}
    a &= s \\
    b &= 0
\end{align}

This perturbation tests for intensity-based feature learning, where larger activations indicate stronger feature presence. If the model is using intensity metric features, performance should decrease as the activation values changes. In either case, we expect performance to drop when activations are scaled small enough to intersect with the distance metric space.

\subsubsection{Perturbation Ranges}
We applied perturbations across carefully selected ranges to capture the full spectrum of behavior, with bounds selected to induce failure conditions:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
\textbf{Perturbation Type} & \textbf{Range} \\
\hline
Distance & $[-2.0, 1.0]$ relative to activation range \\
Intensity & $[0.01, 10.0]$ logarithmic scale \\
\hline
\end{tabular}
\end{table}


\subsection{Evaluation}
For each perturbation configuration, we:
\begin{enumerate}
    \item Apply the perturbation to all nodes in the perturbation layer
    \item Measure the model's accuracy on the training set
    \item Record the accuracy change from baseline
\end{enumerate}

We evaluate on the training set to directly observe how perturbations affect the features the network learned during training. Changes in accuracy indicate the model is using the perturbed feature type, while maintained accuracy suggests those features are not critical to the model's decisions.