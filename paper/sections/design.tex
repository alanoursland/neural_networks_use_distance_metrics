\section{Experimental Design}

To empirically investigate whether neural networks naturally learn distance-based features, we designed systematic perturbation experiments to differentiate between distance-based and intensity-based feature learning. This experimental framework directly compares these two interpretations by examining how learned features respond to specific modifications of their activation patterns. We hypothesize that perturbing the "true representation" will result in a drop in model accuracy. 

We train a basic feedforward model on the MNIST dataset to test our hypotheses. Our goal is to obtain a robust model for perturbation analysis, not to optimize model accuracy. The network processes MNIST digits through the following layers:

\begin{equation}
    x \rightarrow \text{Linear}(784) \rightarrow \text{Perturbation} \rightarrow \text{Activation (ReLU/Abs)} \rightarrow \text{Linear}(10) \rightarrow y
\end{equation}
 
The perturbation layer is a custom module designed to control activation patterns using three fixed parameters: a multiplicative factor ($scale$), a translational offset ($offset$), and a clipping threshold ($clip$). During training, these parameters remain fixed ($scale = 1$, $offset = 0$, $clip = \infty$), ensuring the layer does not influence the network's learning. During perturbation testing, these parameters are modified to probe the network's learned features. For each input $x$, the perturbation layer applies the following operation: $y = \min(scale \cdot x + offset, clip)$, where $scale$, $offset$, and $clip$ are adjustable for each unit.

The model was trained on the entire MNIST dataset (rather than using minibatches) for 5000 epochs using Stochastic Gradient Descent (learning rate = 0.001, loss = cross-entropy). Data normalization used $\mu=0.1307$, $\sigma=0.3081$. To ensure statistically significant results, we repeated each experiment 20 times.
\subsection{Perturbation Design}

The core of our experimental design centers on two distinct perturbation types: one targeting distance-based features and the other targeting intensity-based features.

Distance-based features are expected to lie near the decision boundary. By shifting the decision boundary, we increase the distance between active features and the boundary. If these features are critical for classification, this shift should result in reduced model performance. We also seek to maintain the position of intensity features in this operation. For each node, we calculate the output range, scale by the specified percentage, and then apply the offset as a percentage of the range. The perturbation equation for a given percentage $p$ and range $r$ is: $\{scale = (1-p) \cdot r,\, offset = p \cdot r \}$.

We lack a statistical framework for intensity metrics, so we rely on heuristics to identify perturbations that might disrupt them. Two operations are tested: scaling and clipping. Scaling changes the specific value of the intensity feature, while clipping changes the value and removes the ability to distinguish between multiple intensity features.

Scaling simply multiplies node outputs by a scalar value. Distance-features are affected too, but the change is small since they are small. For a scaling percentage $p$, the perturbation equation is $\{scale = p\}$.

Clipping caps activations at threshold value. This destroys information about relative differences among high-activation features. For a cutoff percentage $p$ and range $r$, the equation is $\{clip = p \cdot r\}$

\subsection{Evaluation}

Perturbation ranges were selected to span a broad spectrum to ensure comprehensive evaluation. The ranges overlap to facilitate direct comparison between distance and intensity metrics. All percentages are applied to individual node ranges over the input set. Intensity and cutoff range over $[1\%..1000\%]$. Offset ranges over $[-200\%..100\%]$.

We select a percentage in the perturbation range, calculate and apply $scale$, $offset$ and $clip$ for the active test, evaluate on the entire training set, and calculate the resulting accuracy. We use the training set, and not the test set, to observe how perturbations affect the features learned during training. Changes in accuracy indicate reliance on the perturbed feature type, while stable accuracy suggests that the features are not critical to the model's decisions. The use of the training set ensures a comprehensive assessment with a sufficient number of data points.
