\section{Experimental Design}

To validate the theoretical connection between neural networks and Mahalanobis distance, we designed a series of controlled experiments examining how networks respond to systematic perturbations. Our experimental framework enables direct comparison between traditional intensity-based interpretations and the proposed distance-based interpretation of neural network features.

\subsection{Model Architecture}
We implemented a minimal architecture that captures the essential components of the theoretical framework while eliminating confounding factors. The base architecture consists of:

\begin{equation}
    f(x) = W_2(g(W_1x + b_1)) + b_2
\end{equation}

** todo: I don't like this model representation. It feels unnecessarily opaque **

where $g(\cdot)$ is either the absolute value function (Abs) or rectified linear unit (ReLU). This architecture includes:

\begin{itemize}
    \item Input layer: Flattened MNIST images ($784$ dimensions)
    \item Hidden layer: $128$ neurons with perturbation capability
    \item Output layer: $10$ neurons (one per digit)
\end{itemize}

The perturbation layer introduces two learnable parameters per neuron:
\begin{equation}
    h(x) = ax + b
\end{equation}

** todo: h(x) should be included in the model definition above **

where $a$ represents scaling (intensity) and $b$ represents shifting (distance). During training, these parameters are initialized to $a=1$ and $b=0$, allowing the network to learn any necessary perturbations.

** todo: This is wrong. The parameters in the perturbation network should not be learned. They are static and initialized to have zero effect on the model. They exist only to allow perturbations after training. ** 

\subsection{Dataset and Training}
We utilized the MNIST dataset for our experiments, chosen for its well-understood properties and minimal preprocessing requirements. The dataset consists of:
\begin{itemize}
    \item Training set: 60,000 examples
    \item Test set: 10,000 examples
    \item Input dimensions: $28 \times 28$ grayscale images
    \item Output: 10 classes (digits 0-9)
\end{itemize}

Data preprocessing followed standard practices:
\begin{equation}
    x_{normalized} = \frac{x - \mu}{\sigma}
\end{equation}
where $\mu = 0.1307$ and $\sigma = 0.3081$ are the empirical mean and standard deviation of the MNIST dataset.

Training configuration:
\begin{itemize}
    \item Optimizer: SGD with learning rate $0.001$
    \item Loss function: Cross-entropy loss
    \item Epochs: $5,000$
    \item Batch size: Full dataset (to eliminate batch normalization effects)
    \item Random seed: $411713593$ for reproducibility
\end{itemize}

** todo: talk about how we aren't doing minibatch learning. We are using the entire training set as a single batch. We run so many epochs because we only have one parameter update per epoch ** 

\subsection{Perturbation Analysis}

We developed two types of perturbation experiments to probe the network's learned representations:

\subsubsection{Intensity Perturbation}
This experiment tests the hypothesis that distance-based features should be invariant to scaling. We modify the perturbation layer parameters:
\begin{equation}
    a_{perturbed} = (1 + \alpha)a_{original}
\end{equation}
where $\alpha \in \{0.0, 0.1, 0.2, 0.4, 0.8, 1.6\}$ represents increasing levels of intensity scaling.

\subsubsection{Distance Perturbation}
This experiment examines sensitivity to shifts in decision boundaries:
\begin{equation}
    b_{perturbed} = b_{original} + \delta
\end{equation}
where $\delta \in \{-0.8, -0.4, -0.2, -0.1, 0.0, 0.1, 0.2, 0.4, 0.8\}$ represents symmetric shifts around the learned boundary.

It's a little more complicated than that. We scale and translate to keep the max activation value the same.

The experimental procedure for each perturbation type:
\begin{enumerate}
    \item Train network to convergence
    \item Apply perturbation to all hidden layer neurons
    \item Evaluate accuracy on test set
    \item Repeat for each perturbation magnitude
\end{enumerate}

\subsection{Evaluation Metrics}

Our primary evaluation metric is classification accuracy on the test set:
\begin{equation}
    \text{Accuracy} = \frac{1}{N}\sum_{i=1}^N \mathbb{1}[\arg\max(f(x_i)) = y_i]
\end{equation}

We analyze the perturbation response patterns through:
\begin{itemize}
    \item Accuracy vs. perturbation magnitude curves
    \item Symmetry analysis of responses
    \item Comparison between Abs and ReLU activations
    \item Statistical significance tests for observed differences
\end{itemize}

** todo: I don't care about symmetry. I don't care about relative performance between abs and relu. I'm testing them both because the original paper discusses both ** 

** todo: I need to do statistical significance. Remind me. I get your help setting that up correctly. ** 

\subsection{Implementation Details}

The experiments were implemented using PyTorch and executed on CUDA-enabled GPUs. Key implementation features include:
\begin{itemize}
    \item Custom perturbation layer with learnable parameters
    \item Full GPU memory management to handle large batch sizes
    \item Checkpoint saving at accuracy thresholds: $\{10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99, 100\}$\%
    \item Multiple random initializations to ensure robustness
\end{itemize}

** todo: The thresholds aren't important for this paper **

Code for reproducing all experiments is available in our public repository\footnote{Repository link will be provided upon publication}.

** todo: github repository is at https://github.com/alanoursland/neural_networks_use_distance_metrics **

\subsection{Expected Outcomes}

Under the Mahalanobis distance interpretation, we expect:
\begin{itemize}
    \item Abs networks to show:
        \begin{itemize}
            \item Invariance to intensity perturbations
            \item Symmetric degradation with distance perturbations
            \item Performance curves resembling Gaussian probability densities
        \end{itemize}
    \item ReLU networks to show:
        \begin{itemize}
            \item Asymmetric responses to perturbations
            \item Different behavior for positive vs. negative shifts
            \item Potential compensation through subsequent layer adaptation
        \end{itemize}
\end{itemize}

** todo: For this paper, we only care about the response to perturbation. We don't care about symmetry or Gaussian curves or compensation by later layers. **

These predictions provide clear criteria for validating the theoretical framework through empirical observation.