\section{Conclusion}

This paper provides empirical validation for the theoretical connection between neural networks and Mahalanobis distance proposed in \citep{oursland2024interpreting}. Through systematic perturbation analysis, we demonstrated that neural networks with different activation functions implement distinct forms of distance-based computation, offering new insights into their learning and decision-making processes.

Our experiments show that both architectures are sensitive to distance perturbations but resistant to intensity perturbations. This supports the idea that neural networks learn through distance-based representations. The Abs network's performance degrades more dramatically with small offsets than the ReLU network's performance. This may be because the Abs network relies on precise distance measurements, while the ReLU network uses a multi-feature approach.

Both architectures maintain consistent performance under scaling perturbations, which appears to support distance-based rather than intensity-based computation. However, the lack of a precise mathematical definition for intensity metrics makes it difficult to definitively rule out intensity-based interpretations. This limitation highlights a broader challenge in the field: we cannot fully disprove a concept that lacks rigorous mathematical formulation.

These results provide empirical support for the theory that linear nodes naturally learn to generate distance metrics. However, more work is needed to strengthen this theoretical framework, particularly in understanding how these distance computations compose through deeper networks and interact across multiple layers. The evidence presented here suggests that distance metrics may provide a more fruitful framework for understanding and interpreting neural networks than traditional intensity-based interpretations.