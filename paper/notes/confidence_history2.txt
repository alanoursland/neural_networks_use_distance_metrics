
1.  The Genesis of "Larger is Stronger"

McCulloch-Pitts Neuron (1943): Although not a learning model, the McCulloch-Pitts neuron introduced the idea of activations as all-or-none signals. This laid the groundwork for associating higher activation with a stronger response to a stimulus.
Perceptron (1958): Rosenblatt's perceptron further solidified this notion by using "1" to represent the positive, feature-detecting state. While binary, it reinforced the idea that larger activation signifies a stronger presence of the feature.
Early MLPs and Backpropagation: Even with the advent of multilayer perceptrons and continuous activation functions, the interpretation of larger activations as more important or salient persisted. This was often reflected in visualizations of activations and analyses of feature maps.
2.  Reinforcement through Deep Learning

ReLU and its Variants: The popularity of ReLU and its variants further strengthened the "larger is stronger" interpretation. The focus on positive activations and the sparsity induced by ReLU implicitly reinforced the idea that larger values carry more information.
Visualization Techniques: Many visualization techniques for deep learning, such as activation maximization and saliency maps, highlight regions with large activations, implicitly promoting this interpretation.
Attention Mechanisms: Attention mechanisms, while providing more nuanced interpretations, often still rely on the magnitude of attention weights, which can be seen as a form of activation strength.
3.  Challenges and Alternative Perspectives

Sparsity and Efficiency: The growing interest in sparsity and efficient neural networks raises questions about the role of zero activations and whether large activations are always necessary for effective representation.
Adversarial Examples: The vulnerability of neural networks to adversarial examples, where small input perturbations can cause large changes in activations and predictions, challenges the robustness of the "larger is stronger" interpretation.
Distance-based Approaches: The success of distance-based methods like RBF networks and Siamese networks suggests that focusing solely on activation magnitude might overlook important relational information encoded in the activation space.





1. The Genesis of "Larger is Stronger"

McCulloch-Pitts Neuron (1943):

"A logical calculus of the ideas immanent in nervous activity" by Warren S. McCulloch and Walter Pitts. This foundational paper introduced the idea of neurons with a threshold for activation, where exceeding the threshold results in a signal (analogous to a strong response).
Perceptron (1958):

"The perceptron: a probabilistic model for information storage and organization in the brain" by Frank Rosenblatt. This seminal work not only introduced the perceptron but also used "1" to represent the positive, feature-detecting state, linking larger activation with feature presence.
Early MLPs and Backpropagation:

"Learning representations by back-propagating errors" by David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. This work popularized backpropagation and enabled the training of deeper networks, indirectly leading to the visualization and analysis of activations in hidden layers, often focusing on larger values.
"Visualizing Higher-Layer Features of a Deep Network" by Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. This paper explored visualizing features learned by higher layers, often highlighting neurons with strong activations.
"Gradient-Based Learning Applied to Document Recognition" by Yann LeCun et al. This influential paper on convolutional neural networks (CNNs) for digit recognition also showcased visualizations of learned features, often emphasizing stronger activations.
2. Reinforcement through Deep Learning

ReLU and its Variants:

"Rectified Linear Units Improve Restricted Boltzmann Machines" by Vinod Nair and Geoffrey E. Hinton. This paper introduced ReLU, which implicitly promoted the focus on large, positive activations due to its sparsity-inducing nature.
"Deep Sparse Rectifier Neural Networks" by Xavier Glorot, Antoine Bordes, and Yoshua Bengio. This work further explored the benefits of ReLU and sparsity, indirectly reinforcing the focus on large activations.
"ImageNet Classification with Deep Convolutional Neural Networks" by Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. This landmark paper on AlexNet, a deep CNN that achieved breakthrough results on ImageNet, further popularized the use of ReLU and showcased the effectiveness of networks with large activations.   
Visualization Techniques:

"Visualizing Image Classification Models and Saliency Maps" by Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. This paper introduced saliency maps, which often highlight input regions that lead to large activations.
"Understanding Neural Networks Through Deep Visualization" by Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. This work presented various visualization techniques for deep neural networks, many of which focused on highlighting strong activations.   
"Learning Deep Features for Discriminative Localization" by Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. This paper proposed Class Activation Mapping (CAM) to visualize discriminative regions in images, often focusing on areas with high activations.
Attention Mechanisms:

"Neural Machine Translation by Jointly Learning to Align and Translate" by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. This paper introduced attention mechanisms, where attention weights can be seen as a form of activation strength, reinforcing the focus on larger values.   
"Attention is All you Need" by Ashish Vaswani et al. This influential paper introduced the Transformer architecture, which heavily relies on attention mechanisms and further popularized the use of attention weights as indicators of importance.
3. Challenges and Alternative Perspectives

Sparsity and Efficiency:

"Learning both Weights and Connections for Efficient Neural Networks" by Song Han, Jeff Pool, John Tran, and William J. Dally. This paper explored pruning techniques to create sparse neural networks, raising questions about the necessity of large activations for all representations.
"Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Kernel" by Tianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu. This work proposed a method for training sparse neural networks from scratch, further challenging the assumption that dense, high-activation networks are always optimal.
Adversarial Examples:

"Explaining and Harnessing Adversarial Examples" by Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. This paper provided insights into adversarial examples and their impact on activations, challenging the robustness of relying solely on activation magnitude.
"Towards Deep Learning Models Resistant to Adversarial Attacks" by Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. This work explored methods to improve the robustness of neural networks to adversarial attacks, recognizing the limitations of relying solely on large activations for interpretation.   
Distance-based Approaches:

"Radial basis functions, multi-variable functional interpolation and adaptive networks" by David S. Broomhead and David Lowe. This paper introduced Radial Basis Function (RBF) networks, which explicitly utilize distance computations.
"Signature verification using a 'Siamese' time delay neural network" by Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. This work introduced Siamese networks, which leverage distance metrics to learn similarity.
"FaceNet: A Unified Embedding for Face Recognition and Clustering"

 by Florian Schroff, Dmitry Kalenichenko, and James Philbin. This paper presented FaceNet, a deep learning model for face recognition that uses a distance metric to learn embeddings, demonstrating the effectiveness of distance-based approaches in complex tasks.

 @article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts,   
 Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}   

}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey   
 E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group   
 UK}
}

@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  number={3},
  pages={1},
  year={2009}   

}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}   

}

@article{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey   
 E},
  journal={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}

@article{glorot2011deep,
  title={Deep sparse rectifier neural networks},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  journal={Proceedings   
 of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011},   

  publisher={JMLR Workshop and Conference Proceedings}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey   
 E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}   

}

@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}   

}

@article{yosinski2015understanding,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015}   

}

@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2929},
  year={2016}   

}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}   

}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{han2015learning,   

  title={Learning both weights and connections for efficient neural networks},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William J},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}

@inproceedings{wang2020dynamic,
  title={Dynamic sparse training: Find efficient sparse network from scratch with trainable masked kernel},
  author={Wang, Tianzhe and Wang, Kuan and Cai, Han and Lin, Ji and Liu, Zhijian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3031--3040},
  year={2020}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}   

}

@inproceedings{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},   

  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{broomhead1988radial,   

  title={Radial basis functions, multi-variable functional interpolation and adaptive networks},
  author={Broomhead, David S and Lowe, David},
  journal={Royal Signals and Radar Establishment Malvern (United Kingdom)},
  year={1988}
}

@inproceedings{bromley1994signature,
  title={Signature verification using a" Siamese" time delay neural network},
  author={Bromley, Jane and Guyon,   
 Isabelle and LeCun, Yann and S{\"a}ckinger, Eduard and Shah, Roopak},
  booktitle={Advances in neural information processing systems},
  pages={737--744},
  year={1994}   

}

@inproceedings{schroff2015facenet,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},   

  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}   

}